diff -Nur linux-2.6.27.official/arch/mips/Kconfig linux-2.6.27/arch/mips/Kconfig
--- linux-2.6.27.official/arch/mips/Kconfig	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/Kconfig	2009-04-29 22:11:28.000000000 +0800
@@ -181,6 +181,28 @@
 	  Lemote Fulong mini-PC board based on the Chinese Loongson-2E CPU and
 	  an FPGA northbridge
 
+config MIPS_CAMELOT
+	bool "Camelot Platform"
+	select BOOT_ELF32
+	select BOOT_RAW
+	#select CEVT_R4K
+	#select CSRC_R4K
+	select DMA_NONCOHERENT
+	#select GENERIC_ISA_DMA
+	select IRQ_CPU
+	#select IRQ_GIC
+	#select HW_HAS_PCI
+	#select SWAP_IO_SPACE
+	#select SYS_HAS_CPU_MIPS32_R1
+	select SYS_HAS_CPU_R3000
+	select SYS_HAS_EARLY_PRINTK
+	select SYS_SUPPORTS_32BIT_KERNEL
+	select SYS_SUPPORTS_BIG_ENDIAN
+	select NO_EXCEPT_FILL
+	select ZONE_DMA
+	help
+	  This enables support for the Camelot based platform.
+
 config MIPS_MALTA
 	bool "MIPS Malta board"
 	select ARCH_MAY_HAVE_PC_FDC
diff -Nur linux-2.6.27.official/arch/mips/kernel/cpu-probe.c linux-2.6.27/arch/mips/kernel/cpu-probe.c
--- linux-2.6.27.official/arch/mips/kernel/cpu-probe.c	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/kernel/cpu-probe.c	2009-04-29 22:11:27.000000000 +0800
@@ -297,6 +297,14 @@
 			c->options |= MIPS_CPU_FPU;
 		c->tlbsize = 64;
 		break;
+#if defined(CONFIG_MIPS_CAMELOT)
+	case PRID_IMP_CAMELOT:
+		c->cputype = CPU_R3000;
+		c->isa_level = MIPS_CPU_ISA_I;
+		c->options = MIPS_CPU_TLB | MIPS_CPU_3K_CACHE;
+		c->tlbsize = 16;
+		break;
+#endif
 	case PRID_IMP_R3000:
 		if ((c->processor_id & 0xff) == PRID_REV_R3000A)
 			if (cpu_has_confreg())
diff -Nur linux-2.6.27.official/arch/mips/kernel/traps.c linux-2.6.27/arch/mips/kernel/traps.c
--- linux-2.6.27.official/arch/mips/kernel/traps.c	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/kernel/traps.c	2009-04-29 22:11:27.000000000 +0800
@@ -1585,6 +1585,9 @@
 	 * configuration.
 	 */
 	set_handler(0x180, &except_vec3_generic, 0x80);
+#if defined(CONFIG_MIPS_CAMELOT)
+	set_handler(0x80, &except_vec3_generic, 0x80);
+#endif
 
 	/*
 	 * Setup default vectors
diff -Nur linux-2.6.27.official/arch/mips/lib/csum_partial.S linux-2.6.27/arch/mips/lib/csum_partial.S
--- linux-2.6.27.official/arch/mips/lib/csum_partial.S	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/lib/csum_partial.S	2009-04-29 22:11:27.000000000 +0800
@@ -238,8 +238,34 @@
 	 andi	t0, a1, 2
 
 	/* Still a full word to go  */
+#if defined(CONFIG_MIPS_CAMELOT)
+	move	t1, zero
+
+	lbu	t2, (src)
+	PTR_ADDIU	src, 1
+	or	t1, t2
+	
+	lbu	t2, (src)
+	sll	t1, t1, 8
+	PTR_ADDIU	src, 1
+	or	t1, t2
+
+	lbu	t2, (src)
+	sll	t1, t1, 8
+	PTR_ADDIU	src, 1
+	or	t1, t2
+
+	lbu	t2, (src)
+	sll	t1, t1, 8
+	PTR_ADDIU	src, 1
+	or	t1, t2
+
+	nop
+#else
 	ulw	t1, (src)
 	PTR_ADDIU	src, 4
+#endif
+
	ADDC(sum, t1)

 1:	move	t1, zero
@@ -250,8 +276,23 @@
 	 andi	t0, a1, 1
 
 	/* Still a halfword to go  */
+#if defined(CONFIG_MIPS_CAMELOT)
+	move	t1, zero
+
+	lbu	t2, (src)
+	PTR_ADDIU	src, 1
+	or	t1, t2
+	
+	lbu	t2, (src)
+	sll	t1, t1, 8
+	PTR_ADDIU	src, 1
+	or	t1, t2
+
+	nop	
+#else
 	ulhu	t1, (src)
 	PTR_ADDIU	src, 2
+#endif
 
 1:	beqz	t0, 1f
 	 sll	t1, t1, 16
@@ -417,6 +458,14 @@
 	 * Note: dst & src may be unaligned, len may be 0
 	 * Temps
 	 */
+#if defined(CONFIG_MIPS_CAMELOT)
+	sltu	t2, len, NBYTES
+	and	t1, dst, ADDRMASK
+	bnez	t2, .Lcopy_bytes_checklen
+	 and	t0, src, ADDRMASK
+	b	.Lcopy_bytes_csum
+	 nop
+#else
 	/*
 	 * The "issue break"s below are very approximate.
 	 * Issue delays for dcache fills will perturb the schedule, as will
@@ -628,6 +677,45 @@
 	bne	len, rem, 1b
 	.set	noreorder
 
+#endif
+#if defined(CONFIG_MIPS_CAMELOT)
+.Lcopy_bytes_csum: 
+#define rem t7	
+	/* len >= NBYTES */
+	 and	rem, len, NBYTES-1	    # rem = len % NBYTES
+	beq	rem, len, .Lcopy_bytes
+	 nop
+1:
+
+EXC(	lbu	t0, 0(src), .Ll_exc_copy)
+	move	t2, zero		    # zero t2 in 1st byte operation
+EXC(	sb	t0, 0(dst), .Ls_exc)
+	or	t2, t0
+
+EXC(	lbu	t0, 1(src), .Ll_exc_copy)
+	sll	t2, t2, 8		    # shift left 8 bits
+EXC(	sb	t0, 1(dst), .Ls_exc)
+	or	t2, t0			    # or 2nd byte
+
+EXC(	lbu	t0, 2(src), .Ll_exc_copy)
+	sll	t2, t2, 8		    # shift left 8 bits
+EXC(	sb	t0, 2(dst), .Ls_exc)
+	or	t2, t0			    # or 3rd byte
+
+EXC(	lbu	t0, 3(src), .Ll_exc_copy)
+	sll	t2, t2, 8		    # shift left 8 bits
+EXC(	sb	t0, 3(dst), .Ls_exc)
+	or	t2, t0			    # get full 4 bytes in t2
+
+	ADD	src, src, NBYTES
+	SUB	len, len, NBYTES
+	ADD 	dst, dst, NBYTES
+
+	ADDC(sum, t2)
+
+	bne	len, rem, 1b
+	nop
+#endif
 .Lcopy_bytes_checklen:
 	beqz	len, .Ldone
 	 nop
diff -Nur linux-2.6.27.official/arch/mips/lib/memcpy-inatomic.S linux-2.6.27/arch/mips/lib/memcpy-inatomic.S
--- linux-2.6.27.official/arch/mips/lib/memcpy-inatomic.S	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/lib/memcpy-inatomic.S	2009-04-29 22:11:27.000000000 +0800
@@ -196,6 +196,21 @@
 	 */
 #define rem t8
 
+#if defined(CONFIG_MIPS_CAMELOT)
+	b	.Lcopy_bytes_checklen
+	nop
+
+	/* unsafe: check and enable aligned copy path later */
+	sltu	t2, len, NBYTES
+	and	t1, dst, ADDRMASK
+	bnez	t2, .Lcopy_bytes_checklen
+	and	t2, src, ADDRMASK
+	bnez	t1, .Lcopy_bytes_checklen
+	nop
+	bnez	t2, .Lcopy_bytes_checklen
+	nop
+#else
+
 	/*
 	 * The "issue break"s below are very approximate.
 	 * Issue delays for dcache fills will perturb the schedule, as will
@@ -220,6 +235,7 @@
 	 * use delay slot for fall-through
 	 * src and dst are aligned; need to compute rem
 	 */
+#endif
 .Lboth_aligned:
 	 SRL	t0, len, LOG_NBYTES+3    	# +3 for 8 units/iter
 	beqz	t0, .Lcleanup_both_aligned	# len < 8*NBYTES
@@ -293,6 +309,7 @@
 	bne	rem, len, 1b
 	.set	noreorder
 
+#if !defined(CONFIG_MIPS_CAMELOT)
 	/*
 	 * src and dst are aligned, need to copy rem bytes (rem < NBYTES)
 	 * A loop would do only a byte at a time with possible branch
@@ -391,6 +408,21 @@
 	bne	len, rem, 1b
 	.set	noreorder
 
+#endif
+#if defined(CONFIG_MIPS_CAMELOT)
+.Lcopy_bytes_checklen:
+	beqz	len, .Ldone
+	nop
+.Lcopy_bytes:
+EXC(	lb	t0, 0(src), .Ll_exc)
+ 	SUB	len, len, 1
+	sb	t0, 0(dst)
+	ADD	dst, dst, 1
+	ADD	src, src, 1
+	bnez	len, .Lcopy_bytes
+	nop
+#else
+
 .Lcopy_bytes_checklen:
 	beqz	len, .Ldone
 	 nop
@@ -414,6 +446,7 @@
 	SUB	len, len, 1
 	jr	ra
 	 sb	t0, NBYTES-2(dst)
+#endif
 .Ldone:
 	jr	ra
 	 nop
diff -Nur linux-2.6.27.official/arch/mips/lib/memcpy.S linux-2.6.27/arch/mips/lib/memcpy.S
--- linux-2.6.27.official/arch/mips/lib/memcpy.S	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/lib/memcpy.S	2009-04-29 22:11:27.000000000 +0800
@@ -200,6 +200,21 @@
 #define rem t8
 
 	R10KCBARRIER(0(ra))
+
+#if defined(CONFIG_MIPS_CAMELOT)
+	b	.Lcopy_bytes_checklen
+	nop
+
+	/* unsafe: check and enable aligned copy path later */ 
+	sltu	t2, len, NBYTES
+	and	t1, dst, ADDRMASK
+	bnez	t2, .Lcopy_bytes_checklen
+	and	t2, src, ADDRMASK
+	bnez	t1, .Lcopy_bytes_checklen
+	nop
+	bnez	t2, .Lcopy_bytes_checklen
+	nop
+#else
 	/*
 	 * The "issue break"s below are very approximate.
 	 * Issue delays for dcache fills will perturb the schedule, as will
@@ -224,6 +239,7 @@
 	 * use delay slot for fall-through
 	 * src and dst are aligned; need to compute rem
 	 */
+#endif
 .Lboth_aligned:
 	 SRL	t0, len, LOG_NBYTES+3    # +3 for 8 units/iter
 	beqz	t0, .Lcleanup_both_aligned # len < 8*NBYTES
@@ -300,6 +316,7 @@
 	bne	rem, len, 1b
 	.set	noreorder
 
+#if !defined(CONFIG_MIPS_CAMELOT)
 	/*
 	 * src and dst are aligned, need to copy rem bytes (rem < NBYTES)
 	 * A loop would do only a byte at a time with possible branch
@@ -401,6 +418,20 @@
 	bne	len, rem, 1b
 	.set	noreorder
 
+#endif
+#if defined(CONFIG_MIPS_CAMELOT)
+.Lcopy_bytes_checklen:
+	beqz	len, .Ldone
+	 nop
+.Lcopy_bytes:
+EXC(	lb	t0, 0(src), .Ll_exc)
+ 	SUB	len, len, 1
+EXC(	sb	t0, 0(dst), .Ls_exc_p1)
+	ADD	dst, dst, 1
+	ADD	src, src, 1
+	bnez	len, .Lcopy_bytes
+	nop
+#else
 .Lcopy_bytes_checklen:
 	beqz	len, .Ldone
 	 nop
@@ -425,6 +456,7 @@
 	SUB	len, len, 1
 	jr	ra
 EXC(	 sb	t0, NBYTES-2(dst), .Ls_exc_p1)
+#endif
 .Ldone:
 	jr	ra
 	 nop
diff -Nur linux-2.6.27.official/arch/mips/lib/memset.S linux-2.6.27/arch/mips/lib/memset.S
--- linux-2.6.27.official/arch/mips/lib/memset.S	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/lib/memset.S	2009-04-29 22:11:27.000000000 +0800
@@ -72,7 +72,12 @@
 
 FEXPORT(__bzero)
 	sltiu		t0, a2, LONGSIZE	/* very small region? */
+#if defined(CONFIG_MIPS_CAMELOT)
+	/* force single byte bzero, optimize later */
+	b		.Lsmall_memset
+#else
 	bnez		t0, .Lsmall_memset
+#endif
 	 andi		t0, a0, LONGMASK	/* aligned? */
 
 #ifndef CONFIG_CPU_DADDI_WORKAROUNDS
diff -Nur linux-2.6.27.official/arch/mips/Makefile linux-2.6.27/arch/mips/Makefile
--- linux-2.6.27.official/arch/mips/Makefile	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/Makefile	2009-04-29 22:11:28.000000000 +0800
@@ -305,6 +305,14 @@
 cflags-$(CONFIG_LEMOTE_FULONG) += -Iinclude/asm-mips/mach-lemote
 
 #
+# Camelot Platform
+#
+core-$(CONFIG_MIPS_CAMELOT)	+= arch/mips/camelot/generic/
+cflags-$(CONFIG_MIPS_CAMELOT)	+= -Iinclude/asm-mips/camelot
+load-$(CONFIG_MIPS_CAMELOT)	+= 0xffffffff80000400
+# all-$(CONFIG_MIPS_CAMELOT)	:= vmlinux.bin
+
+#
 # MIPS Malta board
 #
 core-$(CONFIG_MIPS_MALTA)	+= arch/mips/mti-malta/
diff -Nur linux-2.6.27.official/arch/mips/mm/cache.c linux-2.6.27/arch/mips/mm/cache.c
--- linux-2.6.27.official/arch/mips/mm/cache.c	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/mm/cache.c	2009-04-29 22:11:28.000000000 +0800
@@ -182,6 +182,13 @@
 		tx39_cache_init();
 	}
 
+#if defined(CONFIG_MIPS_CAMELOT)
+	{
+		extern void __weak camelot_cache_init(void);
+
+		camelot_cache_init();
+	}
+#endif
 	setup_protection_map();
 }
 
diff -Nur linux-2.6.27.official/arch/mips/mm/tlbex.c linux-2.6.27/arch/mips/mm/tlbex.c
--- linux-2.6.27.official/arch/mips/mm/tlbex.c	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/mm/tlbex.c	2009-04-29 22:11:28.000000000 +0800
@@ -168,6 +168,9 @@
 	uasm_i_sll(&p, K0, K0, 2);
 	uasm_i_addu(&p, K1, K1, K0);
 	uasm_i_mfc0(&p, K0, C0_CONTEXT);
+#if defined(CONFIG_MIPS_CAMELOT)
+	uasm_i_nop(&p); /* mfc0 delay */
+#endif		
 	uasm_i_lw(&p, K1, 0, K1); /* cp0 delay */
 	uasm_i_andi(&p, K0, K0, 0xffc); /* load delay */
 	uasm_i_addu(&p, K1, K1, K0);
diff -Nur linux-2.6.27.official/arch/mips/mm/tlb-r3k.c linux-2.6.27/arch/mips/mm/tlb-r3k.c
--- linux-2.6.27.official/arch/mips/mm/tlb-r3k.c	2008-10-10 06:13:53.000000000 +0800
+++ linux-2.6.27/arch/mips/mm/tlb-r3k.c	2009-04-29 22:11:28.000000000 +0800
@@ -277,8 +277,43 @@
 	}
 }
 
+#if defined(CONFIG_MIPS_CAMELOT)
+
+static void __init probe_tlb(void)
+{
+    int i;
+	struct cpuinfo_mips *c = &current_cpu_data;
+    unsigned long temp;
+
+    /* not working yet, check later */
+    return;
+
+    c->tlbsize = 8;
+    temp = read_c0_entryhi();
+    for (i=63; i>0; i=i-8)
+    {
+        write_c0_index(i<<8);
+        write_c0_entryhi(0xaaaaaa80);
+        tlb_write_indexed();
+        tlb_read();
+        if (read_c0_entryhi() == 0xaaaaaa80)
+        {
+            c->tlbsize = (i + 1);
+            break;
+        }
+    };
+    write_c0_entryhi(temp);
+    printk("%d entry TLB.\n", c->tlbsize);
+}
+
+#endif
+
 void __cpuinit tlb_init(void)
 {
+#if defined(CONFIG_MIPS_CAMELOT)
+    probe_tlb();
+#endif
+
 	local_flush_tlb_all();
 
 	build_tlb_refill_handler();
--- linux-2.6.27.official/arch/mips/kernel/unaligned.c	2008-07-14 05:51:29.000000000 +0800
+++ linux-2.6.27/arch/mips/kernel/unaligned.c	2009-06-26 17:40:00.000000000 +0800
@@ -186,6 +186,19 @@
 			goto sigbus;
 
 		__asm__ __volatile__ (
+#if defined(CONFIG_MIPS_CAMELOT)
+			"1: lbu $8, 0(%2)\n"
+			"sll $8, $8, 0x18\n"
+			"move %0, $8\n"
+			"lbu $8, 1(%2)\n"
+			"sll $8, $8, 0x10\n"
+			"or %0, %0, $8\n"
+			"2: lbu $8, 2(%2)\n"
+			"sll $8, $8, 8\n"
+			"or %0, %0, $8\n"
+			"lbu $8, 3(%2)\n"
+			"or %0, %0, $8\n"
+#else
 #ifdef __BIG_ENDIAN
 			"1:\tlwl\t%0, (%2)\n"
 			"2:\tlwr\t%0, 3(%2)\n\t"
@@ -194,6 +207,7 @@
 			"1:\tlwl\t%0, 3(%2)\n"
 			"2:\tlwr\t%0, (%2)\n\t"
 #endif
+#endif
 			"li\t%1, 0\n"
 			"3:\t.section\t.fixup,\"ax\"\n\t"
 			"4:\tli\t%1, %3\n\t"
@@ -204,7 +218,12 @@
 			STR(PTR)"\t2b, 4b\n\t"
 			".previous"
 			: "=&r" (value), "=r" (res)
+#if defined(CONFIG_MIPS_CAMELOT)
+			: "r" (addr), "i" (-EFAULT)
+			: "$8");
+#else
 			: "r" (addr), "i" (-EFAULT));
+#endif
 		if (res)
 			goto fault;
 		compute_return_epc(regs);
@@ -374,6 +393,16 @@
 
 		value = regs->regs[insn.i_format.rt];
 		__asm__ __volatile__ (
+#if defined(CONFIG_MIPS_CAMELOT)
+			"move $8, %1\n"
+			"1: sb $8, 3(%2)\n"
+			"srl $8, $8, 8\n"
+			"sb $8, 2(%2)\n"
+			"srl $8, $8, 8\n"
+			"2: sb $8, 1(%2)\n"
+			"srl $8, $8, 8\n"
+			"sb $8, 0(%2)\n"
+#else
 #ifdef __BIG_ENDIAN
 			"1:\tswl\t%1,(%2)\n"
 			"2:\tswr\t%1, 3(%2)\n\t"
@@ -382,6 +411,7 @@
 			"1:\tswl\t%1, 3(%2)\n"
 			"2:\tswr\t%1, (%2)\n\t"
 #endif
+#endif
 			"li\t%0, 0\n"
 			"3:\n\t"
 			".section\t.fixup,\"ax\"\n\t"
@@ -393,7 +423,12 @@
 			STR(PTR)"\t2b, 4b\n\t"
 			".previous"
 		: "=r" (res)
+#if defined(CONFIG_MIPS_CAMELOT)
+		: "r" (value), "r" (addr), "i" (-EFAULT)
+		: "$8");
+#else
 		: "r" (value), "r" (addr), "i" (-EFAULT));
+#endif
 		if (res)
 			goto fault;
 		compute_return_epc(regs);

diff -Naur linux-2.6.24.4/drivers/net/via-velocity.c linux-2.6.24.4-oxnas/drivers/net/via-velocity.c
--- linux-2.6.24.4/drivers/net/via-velocity.c	2008-03-25 02:49:18.000000000 +0800
+++ linux-2.6.24.4-oxnas/drivers/net/via-velocity.c	2009-05-08 16:26:38.000000000 +0800
@@ -72,7 +72,6 @@
 #include <linux/mii.h>
 #include <linux/in.h>
 #include <linux/if_arp.h>
-#include <linux/if_vlan.h>
 #include <linux/ip.h>
 #include <linux/tcp.h>
 #include <linux/udp.h>
@@ -81,170 +80,50 @@
 
 #include "via-velocity.h"
 
+// Default MAC address
+static const u8 DEFAULT_MAC_ADDRESS[] = { 0x00, 0x30, 0xe0, 0x00, 0x00, 0xff };
+static u32 mac_hi=0;
+static u32 mac_lo=0;
+
+#define EXTRA_RX_SKB_SPACE 32
+#define MAX_HW_FRAGMENTS 6
+//#define VELOCITY_ZERO_COPY_SUPPORT
+//#define LOAD_FROM_EEPROM
+
+#define VELOCITY_DESC_IN_SRAM
+
+#ifdef VELOCITY_DESC_IN_SRAM
+#include <asm/arch/desc_alloc.h>
+#endif // VELOCITY_DESC_IN_SRAM
+
+/* Parse netdev kernel cmdline options */
+static int __init do_setup(char *str)
+{
+    int i;
+    int ints[5];    // Hold arg count and four args
+
+    get_options(str, sizeof(ints)/sizeof(int), ints);
+    for (i=1; i<=ints[0]; i++) {
+        switch (i) {
+            case 3:
+                mac_hi = ints[i];
+                break;
+            case 4:
+                mac_lo = ints[i];
+                break;
+            default:
+                break;
+        }
+    }
+    return 0;
+}
+__setup("netdev=",do_setup);
 
 static int velocity_nics = 0;
 static int msglevel = MSG_LEVEL_INFO;
 
-/**
- *	mac_get_cam_mask	-	Read a CAM mask
- *	@regs: register block for this velocity
- *	@mask: buffer to store mask
- *
- *	Fetch the mask bits of the selected CAM and store them into the
- *	provided mask buffer.
- */
-
-static void mac_get_cam_mask(struct mac_regs __iomem * regs, u8 * mask)
-{
-	int i;
-
-	/* Select CAM mask */
-	BYTE_REG_BITS_SET(CAMCR_PS_CAM_MASK, CAMCR_PS1 | CAMCR_PS0, &regs->CAMCR);
-
-	writeb(0, &regs->CAMADDR);
-
-	/* read mask */
-	for (i = 0; i < 8; i++)
-		*mask++ = readb(&(regs->MARCAM[i]));
-
-	/* disable CAMEN */
-	writeb(0, &regs->CAMADDR);
-
-	/* Select mar */
-	BYTE_REG_BITS_SET(CAMCR_PS_MAR, CAMCR_PS1 | CAMCR_PS0, &regs->CAMCR);
-
-}
-
-
-/**
- *	mac_set_cam_mask	-	Set a CAM mask
- *	@regs: register block for this velocity
- *	@mask: CAM mask to load
- *
- *	Store a new mask into a CAM
- */
-
-static void mac_set_cam_mask(struct mac_regs __iomem * regs, u8 * mask)
-{
-	int i;
-	/* Select CAM mask */
-	BYTE_REG_BITS_SET(CAMCR_PS_CAM_MASK, CAMCR_PS1 | CAMCR_PS0, &regs->CAMCR);
-
-	writeb(CAMADDR_CAMEN, &regs->CAMADDR);
-
-	for (i = 0; i < 8; i++) {
-		writeb(*mask++, &(regs->MARCAM[i]));
-	}
-	/* disable CAMEN */
-	writeb(0, &regs->CAMADDR);
-
-	/* Select mar */
-	BYTE_REG_BITS_SET(CAMCR_PS_MAR, CAMCR_PS1 | CAMCR_PS0, &regs->CAMCR);
-}
-
-static void mac_set_vlan_cam_mask(struct mac_regs __iomem * regs, u8 * mask)
-{
-	int i;
-	/* Select CAM mask */
-	BYTE_REG_BITS_SET(CAMCR_PS_CAM_MASK, CAMCR_PS1 | CAMCR_PS0, &regs->CAMCR);
-
-	writeb(CAMADDR_CAMEN | CAMADDR_VCAMSL, &regs->CAMADDR);
-
-	for (i = 0; i < 8; i++) {
-		writeb(*mask++, &(regs->MARCAM[i]));
-	}
-	/* disable CAMEN */
-	writeb(0, &regs->CAMADDR);
-
-	/* Select mar */
-	BYTE_REG_BITS_SET(CAMCR_PS_MAR, CAMCR_PS1 | CAMCR_PS0, &regs->CAMCR);
-}
-
-/**
- *	mac_set_cam	-	set CAM data
- *	@regs: register block of this velocity
- *	@idx: Cam index
- *	@addr: 2 or 6 bytes of CAM data
- *
- *	Load an address or vlan tag into a CAM
- */
-
-static void mac_set_cam(struct mac_regs __iomem * regs, int idx, const u8 *addr)
-{
-	int i;
-
-	/* Select CAM mask */
-	BYTE_REG_BITS_SET(CAMCR_PS_CAM_DATA, CAMCR_PS1 | CAMCR_PS0, &regs->CAMCR);
-
-	idx &= (64 - 1);
-
-	writeb(CAMADDR_CAMEN | idx, &regs->CAMADDR);
-
-	for (i = 0; i < 6; i++) {
-		writeb(*addr++, &(regs->MARCAM[i]));
-	}
-	BYTE_REG_BITS_ON(CAMCR_CAMWR, &regs->CAMCR);
-
-	udelay(10);
-
-	writeb(0, &regs->CAMADDR);
-
-	/* Select mar */
-	BYTE_REG_BITS_SET(CAMCR_PS_MAR, CAMCR_PS1 | CAMCR_PS0, &regs->CAMCR);
-}
-
-static void mac_set_vlan_cam(struct mac_regs __iomem * regs, int idx,
-			     const u8 *addr)
-{
-
-	/* Select CAM mask */
-	BYTE_REG_BITS_SET(CAMCR_PS_CAM_DATA, CAMCR_PS1 | CAMCR_PS0, &regs->CAMCR);
-
-	idx &= (64 - 1);
-
-	writeb(CAMADDR_CAMEN | CAMADDR_VCAMSL | idx, &regs->CAMADDR);
-	writew(*((u16 *) addr), &regs->MARCAM[0]);
-
-	BYTE_REG_BITS_ON(CAMCR_CAMWR, &regs->CAMCR);
-
-	udelay(10);
-
-	writeb(0, &regs->CAMADDR);
-
-	/* Select mar */
-	BYTE_REG_BITS_SET(CAMCR_PS_MAR, CAMCR_PS1 | CAMCR_PS0, &regs->CAMCR);
-}
-
-
-/**
- *	mac_wol_reset	-	reset WOL after exiting low power
- *	@regs: register block of this velocity
- *
- *	Called after we drop out of wake on lan mode in order to
- *	reset the Wake on lan features. This function doesn't restore
- *	the rest of the logic from the result of sleep/wakeup
- */
-
-static void mac_wol_reset(struct mac_regs __iomem * regs)
-{
-
-	/* Turn off SWPTAG right after leaving power mode */
-	BYTE_REG_BITS_OFF(STICKHW_SWPTAG, &regs->STICKHW);
-	/* clear sticky bits */
-	BYTE_REG_BITS_OFF((STICKHW_DS1 | STICKHW_DS0), &regs->STICKHW);
-
-	BYTE_REG_BITS_OFF(CHIPGCR_FCGMII, &regs->CHIPGCR);
-	BYTE_REG_BITS_OFF(CHIPGCR_FCMODE, &regs->CHIPGCR);
-	/* disable force PME-enable */
-	writeb(WOLCFG_PMEOVR, &regs->WOLCFGClr);
-	/* disable power-event config bit */
-	writew(0xFFFF, &regs->WOLCRClr);
-	/* clear power status */
-	writew(0xFFFF, &regs->WOLSRClr);
-}
-
 static int velocity_mii_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd);
-static const struct ethtool_ops velocity_ethtool_ops;
+static struct ethtool_ops velocity_ethtool_ops;
 
 /*
     Define module options
@@ -269,6 +148,15 @@
 #define TX_DESC_DEF     64
 VELOCITY_PARAM(TxDescriptors, "Number of transmit descriptors");
 
+#define VLAN_ID_MIN     0
+#define VLAN_ID_MAX     4095
+#define VLAN_ID_DEF     0
+/* VID_setting[] is used for setting the VID of NIC.
+   0: default VID.
+   1-4094: other VIDs.
+*/
+VELOCITY_PARAM(VID_setting, "802.1Q VLAN ID");
+
 #define RX_THRESH_MIN   0
 #define RX_THRESH_MAX   3
 #define RX_THRESH_DEF   0
@@ -282,7 +170,8 @@
 
 #define DMA_LENGTH_MIN  0
 #define DMA_LENGTH_MAX  7
-#define DMA_LENGTH_DEF  0
+#define DMA_LENGTH_100M_DEF  6
+#define DMA_LENGTH_1000M_DEF 6
 
 /* DMA_length[] is used for controlling the DMA length
    0: 8 DWORDs
@@ -294,7 +183,15 @@
    6: SF(flush till emply)
    7: SF(flush till emply)
 */
-VELOCITY_PARAM(DMA_length, "DMA length");
+VELOCITY_PARAM(DMA_length_100M,  "DMA length 100M");
+VELOCITY_PARAM(DMA_length_1000M, "DMA length 1000M");
+
+#define TAGGING_DEF     0
+/* enable_tagging[] is used for enabling 802.1Q VID tagging.
+   0: disable VID seeting(default).
+   1: enable VID setting.
+*/
+VELOCITY_PARAM(enable_tagging, "Enable 802.1Q tagging");
 
 #define IP_ALIG_DEF     0
 /* IP_byte_align[] is used for IP header DWORD byte aligned
@@ -378,7 +275,7 @@
 static int velocity_open(struct net_device *dev);
 static int velocity_change_mtu(struct net_device *dev, int mtu);
 static int velocity_xmit(struct sk_buff *skb, struct net_device *dev);
-static int velocity_intr(int irq, void *dev_instance);
+static int velocity_intr(int irq, void *dev_instance, struct pt_regs *regs);
 static void velocity_set_multi(struct net_device *dev);
 static struct net_device_stats *velocity_get_stats(struct net_device *dev);
 static int velocity_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);
@@ -401,25 +298,23 @@
 static u32 mii_check_media_mode(struct mac_regs __iomem * regs);
 static u32 check_connection_type(struct mac_regs __iomem * regs);
 static int velocity_set_media_mode(struct velocity_info *vptr, u32 mii_status);
+static void hw_set_mac_address(struct velocity_info *vptr, unsigned char* addr);
+static int set_mac_address(struct net_device *dev, void *p);
 
 #ifdef CONFIG_PM
 
 static int velocity_suspend(struct pci_dev *pdev, pm_message_t state);
 static int velocity_resume(struct pci_dev *pdev);
 
-static DEFINE_SPINLOCK(velocity_dev_list_lock);
-static LIST_HEAD(velocity_dev_list);
-
-#endif
-
-#if defined(CONFIG_PM) && defined(CONFIG_INET)
-
 static int velocity_netdev_event(struct notifier_block *nb, unsigned long notification, void *ptr);
 
 static struct notifier_block velocity_inetaddr_notifier = {
       .notifier_call	= velocity_netdev_event,
 };
 
+static DEFINE_SPINLOCK(velocity_dev_list_lock);
+static LIST_HEAD(velocity_dev_list);
+
 static void velocity_register_notifier(void)
 {
 	register_inetaddr_notifier(&velocity_inetaddr_notifier);
@@ -430,12 +325,12 @@
 	unregister_inetaddr_notifier(&velocity_inetaddr_notifier);
 }
 
-#else
+#else				/* CONFIG_PM */
 
 #define velocity_register_notifier()	do {} while (0)
 #define velocity_unregister_notifier()	do {} while (0)
 
-#endif
+#endif				/* !CONFIG_PM */
 
 /*
  *	Internal board variants. At the moment we have only one
@@ -466,7 +361,7 @@
  *	a pointer a static string valid while the driver is loaded.
  */
 
-static const char __devinit *get_chip_name(enum chip_type chip_id)
+static char __devinit *get_chip_name(enum chip_type chip_id)
 {
 	int i;
 	for (i = 0; chip_info_table[i].name != NULL; i++)
@@ -557,11 +452,11 @@
 	if (val == -1)
 		*opt |= (def ? flag : 0);
 	else if (val < 0 || val > 1) {
-		printk(KERN_NOTICE "%s: the value of parameter %s is invalid, the valid range is (0-1)\n",
+		printk(KERN_NOTICE "%s: the value of parameter %s is invalid, the valid range is (0-1)\n", 
 			devname, name);
 		*opt |= (def ? flag : 0);
 	} else {
-		printk(KERN_INFO "%s: set parameter %s to %s\n",
+		printk(KERN_INFO "%s: set parameter %s to %s\n", 
 			devname, name, val ? "TRUE" : "FALSE");
 		*opt |= (val ? flag : 0);
 	}
@@ -581,10 +476,12 @@
 {
 
 	velocity_set_int_opt(&opts->rx_thresh, rx_thresh[index], RX_THRESH_MIN, RX_THRESH_MAX, RX_THRESH_DEF, "rx_thresh", devname);
-	velocity_set_int_opt(&opts->DMA_length, DMA_length[index], DMA_LENGTH_MIN, DMA_LENGTH_MAX, DMA_LENGTH_DEF, "DMA_length", devname);
+	velocity_set_int_opt(&opts->DMA_length_100M,  DMA_length_100M[index],  DMA_LENGTH_MIN, DMA_LENGTH_MAX, DMA_LENGTH_100M_DEF,  "DMA_length 100M",  devname);
+	velocity_set_int_opt(&opts->DMA_length_1000M, DMA_length_1000M[index], DMA_LENGTH_MIN, DMA_LENGTH_MAX, DMA_LENGTH_1000M_DEF, "DMA_length 1000M", devname);
 	velocity_set_int_opt(&opts->numrx, RxDescriptors[index], RX_DESC_MIN, RX_DESC_MAX, RX_DESC_DEF, "RxDescriptors", devname);
 	velocity_set_int_opt(&opts->numtx, TxDescriptors[index], TX_DESC_MIN, TX_DESC_MAX, TX_DESC_DEF, "TxDescriptors", devname);
-
+	velocity_set_int_opt(&opts->vid, VID_setting[index], VLAN_ID_MIN, VLAN_ID_MAX, VLAN_ID_DEF, "VID_setting", devname);
+	velocity_set_bool_opt(&opts->flags, enable_tagging[index], TAGGING_DEF, VELOCITY_FLAGS_TAGGING, "enable_tagging", devname);
 	velocity_set_bool_opt(&opts->flags, txcsum_offload[index], TX_CSUM_DEF, VELOCITY_FLAGS_TX_CSUM, "txcsum_offload", devname);
 	velocity_set_int_opt(&opts->flow_cntl, flow_control[index], FLOW_CNTL_MIN, FLOW_CNTL_MAX, FLOW_CNTL_DEF, "flow_control", devname);
 	velocity_set_bool_opt(&opts->flags, IP_byte_align[index], IP_ALIG_DEF, VELOCITY_FLAGS_IP_ALIGN, "IP_byte_align", devname);
@@ -690,6 +561,61 @@
 }
 
 /**
+ * Cause the eeprom to be read into the chip.
+ * @returns Zero on success
+ */
+int mac_eeprom_reload(struct mac_regs __iomem *regs)
+{
+#if 0
+{
+int i;
+unsigned char __iomem *ptr = (unsigned char __iomem *)regs;
+printk("Before eeprom load regs:\n");
+printk("0x00:\t");
+for (i=0; i <= MAC_REG_BYTEMSK3_3; ++i) {
+    printk("0x%02x\t", readb(&(ptr[i])));
+    if (!((i+1) % 8)) {
+        printk("\n0x%02x:\t", i+1);
+    }
+}
+}
+#endif
+    BYTE_REG_BITS_ON(EECSR_RELOAD, &((regs)->EECSR));
+    mdelay(100);
+#if 0
+{
+int i;
+unsigned char __iomem *ptr = (unsigned char __iomem *)regs;
+printk("\nAfter eeprom load regs:\n");
+printk("0x00:\t");
+for (i=0; i <= MAC_REG_BYTEMSK3_3; ++i) {
+    printk("0x%02x\t", readb(&(ptr[i])));
+    if (!((i+1) % 8)) {
+        printk("\n0x%02x:\t", i+1);
+    }
+}
+}
+#endif
+    return BYTE_REG_BITS_IS_ON(EECSR_RELOAD, &((regs)->EECSR));
+}
+
+
+static inline void mac_set_dma_length(struct velocity_info *vptr)
+{
+	struct mac_regs __iomem *regs = vptr->mac_regs;
+    int burst_size = vptr->options.DMA_length_100M;
+
+    if (!(vptr->mii_status & VELOCITY_LINK_FAIL) &&
+         (vptr->options.spd_dpx == SPD_DPX_AUTO) &&
+         (vptr->mii_status & VELOCITY_SPEED_1000)) {
+        burst_size = vptr->options.DMA_length_1000M;
+    }
+
+//printk("Setting fifo burst size to %d\n", burst_size);
+	BYTE_REG_BITS_SET(burst_size, 0x07, &(regs->DCFG));
+}
+
+/**
  *	velocity_init_registers	-	initialise MAC registers
  *	@vptr: velocity to init
  *	@type: type of initialisation (hot or cold)
@@ -698,7 +624,7 @@
  *	hardware.
  */
 
-static void velocity_init_registers(struct velocity_info *vptr,
+static void velocity_init_registers(struct velocity_info *vptr, 
 				    enum velocity_init_type type)
 {
 	struct mac_regs __iomem * regs = vptr->mac_regs;
@@ -726,11 +652,12 @@
 				netif_wake_queue(vptr->dev);
 		}
 
+		mac_set_dma_length(vptr);
 		enable_flow_control_ability(vptr);
 
 		mac_clear_isr(regs);
 		writel(CR0_STOP, &regs->CR0Clr);
-		writel((CR0_DPOLL | CR0_TXON | CR0_RXON | CR0_STRT),
+		writel((CR0_DPOLL | CR0_TXON | CR0_RXON | CR0_STRT), 
 							&regs->CR0Set);
 
 		break;
@@ -743,16 +670,27 @@
 		velocity_soft_reset(vptr);
 		mdelay(5);
 
+#ifdef LOAD_FROM_EEPROM
 		mac_eeprom_reload(regs);
+#endif // LOAD_FROM_EEPROM
+
 		for (i = 0; i < 6; i++) {
 			writeb(vptr->dev->dev_addr[i], &(regs->PAR[i]));
 		}
+
+        // Initialise the hardware's record of our primary MAC address
+        hw_set_mac_address(vptr, vptr->dev->dev_addr);
+
+        /*
+         *  Set LED Select bits to CASE_1
+         */
+        BYTE_REG_BITS_SET(CFGA_PHYLEDS1, (CFGA_PHYLEDS1 | CFGA_PHYLEDS0), &(regs->CFGA));
+
 		/*
 		 *	clear Pre_ACPI bit.
 		 */
 		BYTE_REG_BITS_OFF(CFGA_PACPI, &(regs->CFGA));
 		mac_set_rx_thresh(regs, vptr->options.rx_thresh);
-		mac_set_dma_length(regs, vptr->options.DMA_length);
 
 		writeb(WOLCFG_SAM | WOLCFG_SAB, &regs->WOLCFGSet);
 		/*
@@ -805,7 +743,9 @@
 				netif_wake_queue(vptr->dev);
 		}
 
+		mac_set_dma_length(vptr);
 		enable_flow_control_ability(vptr);
+
 		mac_hw_mibs_init(regs);
 		mac_write_int_mask(vptr->int_mask, regs);
 		mac_clear_isr(regs);
@@ -866,7 +806,7 @@
 	 * can support more than MAX_UNITS.
 	 */
 	if (velocity_nics >= MAX_UNITS) {
-		dev_notice(&pdev->dev, "already found %d NICs.\n",
+		dev_notice(&pdev->dev, "already found %d NICs.\n", 
 			   velocity_nics);
 		return -ENODEV;
 	}
@@ -876,15 +816,16 @@
 		dev_err(&pdev->dev, "allocate net device failed.\n");
 		goto out;
 	}
-
+	
 	/* Chain it all together */
-
+	
+	SET_MODULE_OWNER(dev);
 	SET_NETDEV_DEV(dev, &pdev->dev);
 	vptr = netdev_priv(dev);
 
 
 	if (first) {
-		printk(KERN_INFO "%s Ver. %s\n",
+		printk(KERN_INFO "%s Ver. %s\n", 
 			VELOCITY_FULL_DRV_NAM, VELOCITY_VERSION);
 		printk(KERN_INFO "Copyright (c) 2002, 2003 VIA Networking Technologies, Inc.\n");
 		printk(KERN_INFO "Copyright (c) 2004 Red Hat Inc.\n");
@@ -898,7 +839,7 @@
 	dev->irq = pdev->irq;
 
 	ret = pci_enable_device(pdev);
-	if (ret < 0)
+	if (ret < 0) 
 		goto err_free_dev;
 
 	ret = velocity_get_pci_info(vptr, pdev);
@@ -928,19 +869,32 @@
 	for (i = 0; i < 6; i++)
 		dev->dev_addr[i] = readb(&regs->PAR[i]);
 
+    // Tell the kernel of our MAC address
+    if ((mac_hi==0)&&(mac_lo==0)) {
+        memcpy(dev->dev_addr, DEFAULT_MAC_ADDRESS, dev->addr_len);
+    } else {
+        int i;
+        for (i=0; i < dev->addr_len; i++) {
+            if (i < sizeof(u32)) {
+                dev->dev_addr[i] = ((mac_hi >> (((sizeof(u32)-1)-i)*8)) & 0xff);
+            } else {
+                dev->dev_addr[i] = ((mac_lo >> (((sizeof(u32)+1)-i)*8)) & 0xff);
+            }
+        }
+    }
 
 	velocity_get_options(&vptr->options, velocity_nics, dev->name);
 
-	/*
+	/* 
 	 *	Mask out the options cannot be set to the chip
 	 */
-
+	 
 	vptr->options.flags &= info->flags;
 
 	/*
 	 *	Enable the chip specified capbilities
 	 */
-
+	 
 	vptr->flags = vptr->options.flags | (info->flags & 0xFF000000UL);
 
 	vptr->wol_opts = vptr->options.wol_opts;
@@ -978,9 +929,9 @@
 
 	velocity_print_info(vptr);
 	pci_set_drvdata(pdev, dev);
-
+	
 	/* and leave the chip powered down */
-
+	
 	pci_set_power_state(pdev, PCI_D3hot);
 #ifdef CONFIG_PM
 	{
@@ -1019,9 +970,9 @@
 	struct net_device *dev = vptr->dev;
 
 	printk(KERN_INFO "%s: %s\n", dev->name, get_chip_name(vptr->chip_id));
-	printk(KERN_INFO "%s: Ethernet Address: %2.2X:%2.2X:%2.2X:%2.2X:%2.2X:%2.2X\n",
-		dev->name,
-		dev->dev_addr[0], dev->dev_addr[1], dev->dev_addr[2],
+	printk(KERN_INFO "%s: Ethernet Address: %2.2X:%2.2X:%2.2X:%2.2X:%2.2X:%2.2X\n", 
+		dev->name, 
+		dev->dev_addr[0], dev->dev_addr[1], dev->dev_addr[2], 
 		dev->dev_addr[3], dev->dev_addr[4], dev->dev_addr[5]);
 }
 
@@ -1044,7 +995,6 @@
 	vptr->pdev = pdev;
 	vptr->chip_id = info->chip_id;
 	vptr->num_txq = info->txqueue;
-	vptr->multicast_limit = MCAM_SIZE;
 	spin_lock_init(&vptr->lock);
 	INIT_LIST_HEAD(&vptr->list);
 }
@@ -1061,12 +1011,12 @@
 static int __devinit velocity_get_pci_info(struct velocity_info *vptr, struct pci_dev *pdev)
 {
 	vptr->rev_id = pdev->revision;
-
+		
 	pci_set_master(pdev);
 
 	vptr->ioaddr = pci_resource_start(pdev, 0);
 	vptr->memaddr = pci_resource_start(pdev, 1);
-
+	
 	if (!(pci_resource_flags(pdev, 0) & IORESOURCE_IO)) {
 		dev_err(&pdev->dev,
 			   "region #0 is not an I/O resource, aborting.\n");
@@ -1105,20 +1055,25 @@
 	u8 *pool;
 
 	/*
-	 *	Allocate all RD/TD rings a single pool
+	 *	Allocate all RD/TD rings a single pool 
 	 */
-
-	psize = vptr->options.numrx * sizeof(struct rx_desc) +
+	 
+	psize = vptr->options.numrx * sizeof(struct rx_desc) + 
 		vptr->options.numtx * sizeof(struct tx_desc) * vptr->num_txq;
 
 	/*
 	 * pci_alloc_consistent() fulfills the requirement for 64 bytes
 	 * alignment
 	 */
-	pool = pci_alloc_consistent(vptr->pdev, psize, &pool_dma);
+#ifdef VELOCITY_DESC_IN_SRAM
+    pool = (u8*)GMAC_DESC_ALLOC_START;
+    pool_dma = GMAC_DESC_ALLOC_START_PA;
+#else
+     pool = pci_alloc_consistent(vptr->pdev, psize, &pool_dma);
+#endif // VELOCITY_DESC_IN_SRAM
 
 	if (pool == NULL) {
-		printk(KERN_ERR "%s : DMA memory allocation failed.\n",
+		printk(KERN_ERR "%s : DMA memory allocation failed.\n", 
 					vptr->dev->name);
 		return -ENOMEM;
 	}
@@ -1130,13 +1085,15 @@
 	vptr->rd_pool_dma = pool_dma;
 
 	tsize = vptr->options.numtx * PKT_BUF_SZ * vptr->num_txq;
-	vptr->tx_bufs = pci_alloc_consistent(vptr->pdev, tsize,
+	vptr->tx_bufs = pci_alloc_consistent(vptr->pdev, tsize, 
 						&vptr->tx_bufs_dma);
 
 	if (vptr->tx_bufs == NULL) {
-		printk(KERN_ERR "%s: DMA memory allocation failed.\n",
+		printk(KERN_ERR "%s: DMA memory allocation failed.\n", 
 					vptr->dev->name);
+#ifndef VELOCITY_DESC_IN_SRAM
 		pci_free_consistent(vptr->pdev, psize, pool, pool_dma);
+#endif // !VELOCITY_DESC_IN_SRAM
 		return -ENOMEM;
 	}
 
@@ -1167,10 +1124,12 @@
 {
 	int size;
 
-	size = vptr->options.numrx * sizeof(struct rx_desc) +
+	size = vptr->options.numrx * sizeof(struct rx_desc) + 
 	       vptr->options.numtx * sizeof(struct tx_desc) * vptr->num_txq;
 
+#ifndef VELOCITY_DESC_IN_SRAM
 	pci_free_consistent(vptr->pdev, size, vptr->rd_ring, vptr->rd_pool_dma);
+#endif // !VELOCITY_DESC_IN_SRAM
 
 	size = vptr->options.numtx * PKT_BUF_SZ * vptr->num_txq;
 
@@ -1219,7 +1178,7 @@
 				break;
 		}
 		done++;
-		dirty = (dirty < vptr->options.numrx - 1) ? dirty + 1 : 0;
+		dirty = (dirty < vptr->options.numrx - 1) ? dirty + 1 : 0;	
 	} while (dirty != vptr->rd_curr);
 
 	if (done) {
@@ -1241,15 +1200,14 @@
 
 static int velocity_init_rd_ring(struct velocity_info *vptr)
 {
-	int ret;
-	int mtu = vptr->dev->mtu;
-
-	vptr->rx_buf_sz = (mtu <= ETH_DATA_LEN) ? PKT_BUF_SZ : mtu + 32;
+	int ret = -ENOMEM;
+	unsigned int rsize = sizeof(struct velocity_rd_info) * 
+					vptr->options.numrx;
 
-	vptr->rd_info = kcalloc(vptr->options.numrx,
-				sizeof(struct velocity_rd_info), GFP_KERNEL);
-	if (!vptr->rd_info)
-		return -ENOMEM;
+	vptr->rd_info = kmalloc(rsize, GFP_KERNEL);
+	if(vptr->rd_info == NULL)
+		goto out;
+	memset(vptr->rd_info, 0, rsize);
 
 	vptr->rd_filled = vptr->rd_dirty = vptr->rd_curr = 0;
 
@@ -1259,7 +1217,7 @@
 			"%s: failed to allocate RX buffer.\n", vptr->dev->name);
 		velocity_free_rd_ring(vptr);
 	}
-
+out:
 	return ret;
 }
 
@@ -1306,26 +1264,28 @@
  *	Returns zero on success or a negative posix errno code for
  *	failure.
  */
-
+ 
 static int velocity_init_td_ring(struct velocity_info *vptr)
 {
 	int i, j;
 	dma_addr_t curr;
 	struct tx_desc *td;
 	struct velocity_td_info *td_info;
+	unsigned int tsize = sizeof(struct velocity_td_info) * 
+					vptr->options.numtx;
 
 	/* Init the TD ring entries */
 	for (j = 0; j < vptr->num_txq; j++) {
 		curr = vptr->td_pool_dma[j];
 
-		vptr->td_infos[j] = kcalloc(vptr->options.numtx,
-					    sizeof(struct velocity_td_info),
-					    GFP_KERNEL);
-		if (!vptr->td_infos[j])	{
+		vptr->td_infos[j] = kmalloc(tsize, GFP_KERNEL);
+		if(vptr->td_infos[j] == NULL)
+		{
 			while(--j >= 0)
 				kfree(vptr->td_infos[j]);
 			return -ENOMEM;
 		}
+		memset(vptr->td_infos[j], 0, tsize);
 
 		for (i = 0; i < vptr->options.numtx; i++, curr += sizeof(struct tx_desc)) {
 			td = &(vptr->td_rings[j][i]);
@@ -1340,31 +1300,12 @@
 	return 0;
 }
 
-/*
- *	FIXME: could we merge this with velocity_free_tx_buf ?
- */
-
-static void velocity_free_td_ring_entry(struct velocity_info *vptr,
-							 int q, int n)
+static void velocity_free_td_ring_entry(struct velocity_info *vptr, int q, int n)
 {
 	struct velocity_td_info * td_info = &(vptr->td_infos[q][n]);
-	int i;
 
-	if (td_info == NULL)
-		return;
-
-	if (td_info->skb) {
-		for (i = 0; i < td_info->nskb_dma; i++)
-		{
-			if (td_info->skb_dma[i]) {
-				pci_unmap_single(vptr->pdev, td_info->skb_dma[i],
-					td_info->skb->len, PCI_DMA_TODEVICE);
-				td_info->skb_dma[i] = (dma_addr_t) NULL;
-			}
-		}
-		dev_kfree_skb(td_info->skb);
-		td_info->skb = NULL;
-	}
+	if (td_info && td_info->skb)
+        velocity_free_tx_buf(vptr, td_info);
 }
 
 /**
@@ -1374,17 +1315,16 @@
  *	Free up the transmit ring for this particular velocity adapter.
  *	We free the ring contents but not the ring itself.
  */
-
+ 
 static void velocity_free_td_ring(struct velocity_info *vptr)
 {
 	int i, j;
 
-	for (j = 0; j < vptr->num_txq; j++) {
+	for (j=0; j < vptr->num_txq; j++) {
 		if (vptr->td_infos[j] == NULL)
 			continue;
-		for (i = 0; i < vptr->options.numtx; i++) {
+		for (i=0; i < vptr->options.numtx; i++) {
 			velocity_free_td_ring_entry(vptr, j, i);
-
 		}
 		kfree(vptr->td_infos[j]);
 		vptr->td_infos[j] = NULL;
@@ -1400,14 +1340,14 @@
  *	any received packets from the receive queue. Hand the ring
  *	slots back to the adapter for reuse.
  */
-
+ 
 static int velocity_rx_srv(struct velocity_info *vptr, int status)
 {
 	struct net_device_stats *stats = &vptr->stats;
 	int rd_curr = vptr->rd_curr;
 	int works = 0;
 
-	do {
+    while (1) {
 		struct rx_desc *rd = vptr->rd_ring + rd_curr;
 
 		if (!vptr->rd_info[rd_curr].skb)
@@ -1440,7 +1384,9 @@
 		rd_curr++;
 		if (rd_curr >= vptr->options.numrx)
 			rd_curr = 0;
-	} while (++works <= 15);
+
+        ++works;
+    }
 
 	vptr->rd_curr = rd_curr;
 
@@ -1461,14 +1407,14 @@
  *	Process the status bits for the received packet and determine
  *	if the checksum was computed and verified by the hardware
  */
-
+ 
 static inline void velocity_rx_csum(struct rx_desc *rd, struct sk_buff *skb)
 {
 	skb->ip_summed = CHECKSUM_NONE;
 
 	if (rd->rdesc1.CSM & CSM_IPKT) {
 		if (rd->rdesc1.CSM & CSM_IPOK) {
-			if ((rd->rdesc1.CSM & CSM_TCPKT) ||
+			if ((rd->rdesc1.CSM & CSM_TCPKT) || 
 					(rd->rdesc1.CSM & CSM_UDPKT)) {
 				if (!(rd->rdesc1.CSM & CSM_TUPOK)) {
 					return;
@@ -1499,20 +1445,19 @@
 	if (pkt_size < rx_copybreak) {
 		struct sk_buff *new_skb;
 
-		new_skb = dev_alloc_skb(pkt_size + 2);
+        /* Always realign IP header to quad boundary if copying anyway */
+		new_skb = dev_alloc_skb(pkt_size + NET_IP_ALIGN);
 		if (new_skb) {
 			new_skb->dev = vptr->dev;
 			new_skb->ip_summed = rx_skb[0]->ip_summed;
 
-			if (vptr->flags & VELOCITY_FLAGS_IP_ALIGN)
-				skb_reserve(new_skb, 2);
+			skb_reserve(new_skb, NET_IP_ALIGN);
 
-			skb_copy_from_linear_data(rx_skb[0], new_skb->data,
-						  pkt_size);
+			memcpy(new_skb->data, rx_skb[0]->data, pkt_size);
 			*rx_skb = new_skb;
 			ret = 0;
 		}
-
+		
 	}
 	return ret;
 }
@@ -1529,13 +1474,10 @@
 static inline void velocity_iph_realign(struct velocity_info *vptr,
 					struct sk_buff *skb, int pkt_size)
 {
-	/* FIXME - memmove ? */
 	if (vptr->flags & VELOCITY_FLAGS_IP_ALIGN) {
-		int i;
-
-		for (i = pkt_size; i >= 0; i--)
-			*(skb->data + i + 2) = *(skb->data + i);
-		skb_reserve(skb, 2);
+//printk("velocity_iph_realign()\n");
+        memmove(skb->data + NET_IP_ALIGN, skb->data, pkt_size);
+		skb_reserve(skb, NET_IP_ALIGN);
 	}
 }
 
@@ -1543,11 +1485,11 @@
  *	velocity_receive_frame	-	received packet processor
  *	@vptr: velocity we are handling
  *	@idx: ring index
- *
+ *	
  *	A packet has arrived. We process the packet and if appropriate
  *	pass the frame up the network stack
  */
-
+ 
 static int velocity_receive_frame(struct velocity_info *vptr, int idx)
 {
 	void (*pci_action)(struct pci_dev *, dma_addr_t, size_t, int);
@@ -1558,7 +1500,7 @@
 	struct sk_buff *skb;
 
 	if (rd->rdesc0.RSR & (RSR_STP | RSR_EDP)) {
-		VELOCITY_PRT(MSG_LEVEL_VERBOSE, KERN_ERR " %s : the received frame span multple RDs.\n", vptr->dev->name);
+		VELOCITY_PRT(MSG_LEVEL_VERBOSE, KERN_ERR " %s : the received frame spans multple RDs.\n", vptr->dev->name);
 		stats->rx_length_errors++;
 		return -EINVAL;
 	}
@@ -1567,6 +1509,7 @@
 		vptr->stats.multicast++;
 
 	skb = rd_info->skb;
+	skb->dev = vptr->dev;
 
 	pci_dma_sync_single_for_cpu(vptr->pdev, rd_info->skb_dma,
 				    vptr->rx_buf_sz, PCI_DMA_FROMDEVICE);
@@ -1574,7 +1517,6 @@
 	/*
 	 *	Drop frame not meeting IEEE 802.3
 	 */
-
 	if (vptr->flags & VELOCITY_FLAGS_VAL_PKT_LEN) {
 		if (rd->rdesc0.RSR & RSR_RL) {
 			stats->rx_length_errors++;
@@ -1614,7 +1558,7 @@
  *	requires *64* byte alignment of the buffer which makes life
  *	less fun than would be ideal.
  */
-
+ 
 static int velocity_alloc_rx_buf(struct velocity_info *vptr, int idx)
 {
 	struct rx_desc *rd = &(vptr->rd_ring[idx]);
@@ -1631,11 +1575,10 @@
 	skb_reserve(rd_info->skb, (unsigned long) rd_info->skb->data & 63);
 	rd_info->skb->dev = vptr->dev;
 	rd_info->skb_dma = pci_map_single(vptr->pdev, rd_info->skb->data, vptr->rx_buf_sz, PCI_DMA_FROMDEVICE);
-
+	
 	/*
 	 *	Fill in the descriptor to match
- 	 */
-
+ 	 */	
 	*((u32 *) & (rd->rdesc0)) = 0;
 	rd->len = cpu_to_le32(vptr->rx_buf_sz);
 	rd->inten = 1;
@@ -1651,9 +1594,9 @@
  *
  *	Scan the queues looking for transmitted packets that
  *	we can complete and clean up. Update any statistics as
- *	necessary/
+ *	neccessary/
  */
-
+ 
 static int velocity_tx_srv(struct velocity_info *vptr, u32 status)
 {
 	struct tx_desc *td;
@@ -1665,7 +1608,7 @@
 	struct net_device_stats *stats = &vptr->stats;
 
 	for (qnum = 0; qnum < vptr->num_txq; qnum++) {
-		for (idx = vptr->td_tail[qnum]; vptr->td_used[qnum] > 0;
+		for (idx = vptr->td_tail[qnum]; vptr->td_used[qnum] > 0; 
 			idx = (idx + 1) % vptr->options.numtx) {
 
 			/*
@@ -1677,8 +1620,7 @@
 			if (td->tdesc0.owner == OWNED_BY_NIC)
 				break;
 
-			if ((works++ > 15))
-				break;
+            ++works;
 
 			if (td->tdesc0.TSR & TSR0_TERR) {
 				stats->tx_errors++;
@@ -1730,7 +1672,7 @@
 	if (vptr->mii_status & VELOCITY_LINK_FAIL) {
 		VELOCITY_PRT(MSG_LEVEL_INFO, KERN_NOTICE "%s: failed to detect cable link\n", vptr->dev->name);
 	} else if (vptr->options.spd_dpx == SPD_DPX_AUTO) {
-		VELOCITY_PRT(MSG_LEVEL_INFO, KERN_NOTICE "%s: Link auto-negotiation", vptr->dev->name);
+		VELOCITY_PRT(MSG_LEVEL_INFO, KERN_NOTICE "%s: Link autonegation", vptr->dev->name);
 
 		if (vptr->mii_status & VELOCITY_SPEED_1000)
 			VELOCITY_PRT(MSG_LEVEL_INFO, " speed 1000M bps");
@@ -1765,17 +1707,48 @@
 }
 
 /**
+ *	velocity_update_hw_mibs	-	fetch MIB counters from chip
+ *	@vptr: velocity to update
+ *
+ *	The velocity hardware keeps certain counters in the hardware
+ * 	side. We need to read these when the user asks for statistics
+ *	or when they overflow (causing an interrupt). The read of the
+ *	statistic clears it, so we keep running master counters in user
+ *	space.
+ */
+static void velocity_update_hw_mibs(struct velocity_info *vptr)
+{
+	int i;
+
+	/* Toggle flush to update MIB SRAM from fast counters */
+	BYTE_REG_BITS_ON(MIBCR_MIBFLSH, &(vptr->mac_regs->MIBCR));
+	while (BYTE_REG_BITS_IS_ON(MIBCR_MIBFLSH, &(vptr->mac_regs->MIBCR)));
+
+	/* Toggle MIBINI to begin MIB SRAM read out. Datasheet says always reads as
+       zero, so no point polling for it to return to zero after setting */
+	BYTE_REG_BITS_ON(MIBCR_MPTRINI, &(vptr->mac_regs->MIBCR));
+
+	for (i=0; i < HW_MIB_SIZE; ++i) {
+		/* Read MIB, preserving both index and count */
+		u32 mib = readl(&(vptr->mac_regs->MIBData));
+		int index = (mib & 0xff000000) >> 24;
+		u32 count =  mib & 0x00ffffff;
+		vptr->mib_counter[index] += count;
+	}
+}
+
+/**
  *	velocity_error	-	handle error from controller
  *	@vptr: velocity
  *	@status: card status
  *
  *	Process an error report from the hardware and attempt to recover
- *	the card itself. At the moment we cannot recover from some
+ *	the card itself. At the moment we cannot recover from some 
  *	theoretically impossible errors but this could be fixed using
  *	the pci_device_failed logic to bounce the hardware
  *
  */
-
+ 
 static void velocity_error(struct velocity_info *vptr, int status)
 {
 
@@ -1786,7 +1759,7 @@
 		BYTE_REG_BITS_ON(TXESR_TDSTR, &regs->TXESR);
 		writew(TRDCSR_RUN, &regs->TDCSRClr);
 		netif_stop_queue(vptr->dev);
-
+		
 		/* FIXME: port over the pci_device_failed code and use it
 		   here */
 	}
@@ -1799,7 +1772,7 @@
 			vptr->mii_status = check_connection_type(regs);
 
 			/*
-			 *	If it is a 3119, disable frame bursting in
+			 *	If it is a 3119, disable frame bursting in 
 			 *	halfduplex mode and enable it in fullduplex
 			 *	 mode
 			 */
@@ -1832,13 +1805,15 @@
 		}
 
 		velocity_print_link_status(vptr);
+
+		mac_set_dma_length(vptr);
 		enable_flow_control_ability(vptr);
 
 		/*
-		 *	Re-enable auto-polling because SRCI will disable
+		 *	Re-enable auto-polling because SRCI will disable 
 		 *	auto-polling
 		 */
-
+		 
 		enable_mii_autopoll(regs);
 
 		if (vptr->mii_status & VELOCITY_LINK_FAIL)
@@ -1846,9 +1821,11 @@
 		else
 			netif_wake_queue(vptr->dev);
 
-	};
+	}
+
 	if (status & ISR_MIBFI)
 		velocity_update_hw_mibs(vptr);
+
 	if (status & ISR_LSTEI)
 		mac_rx_queue_wake(vptr->mac_regs);
 }
@@ -1895,12 +1878,14 @@
  *	All the ring allocation and set up is done on open for this
  *	adapter to minimise memory usage when inactive
  */
-
+ 
 static int velocity_open(struct net_device *dev)
 {
 	struct velocity_info *vptr = netdev_priv(dev);
 	int ret;
 
+    vptr->rx_buf_sz = dev->mtu + NET_IP_ALIGN + EXTRA_RX_SKB_SPACE;
+
 	ret = velocity_init_rings(vptr);
 	if (ret < 0)
 		goto out;
@@ -1912,13 +1897,13 @@
 	ret = velocity_init_td_ring(vptr);
 	if (ret < 0)
 		goto err_free_rd_ring;
-
-	/* Ensure chip is running */
+	
+	/* Ensure chip is running */	
 	pci_set_power_state(vptr->pdev, PCI_D0);
-
+	
 	velocity_init_registers(vptr, VELOCITY_INIT_COLD);
 
-	ret = request_irq(vptr->pdev->irq, &velocity_intr, IRQF_SHARED,
+	ret = request_irq(vptr->pdev->irq, &velocity_intr, SA_SHIRQ,
 			  dev->name, dev);
 	if (ret < 0) {
 		/* Power down the chip */
@@ -1941,7 +1926,7 @@
 	goto out;
 }
 
-/**
+/** 
  *	velocity_change_mtu	-	MTU change callback
  *	@dev: network device
  *	@new_mtu: desired MTU
@@ -1950,7 +1935,7 @@
  *	this interface. It gets called on a change by the network layer.
  *	Return zero for success or negative posix error code.
  */
-
+ 
 static int velocity_change_mtu(struct net_device *dev, int new_mtu)
 {
 	struct velocity_info *vptr = netdev_priv(dev);
@@ -1959,16 +1944,11 @@
 	int ret = 0;
 
 	if ((new_mtu < VELOCITY_MIN_MTU) || new_mtu > (VELOCITY_MAX_MTU)) {
-		VELOCITY_PRT(MSG_LEVEL_ERR, KERN_NOTICE "%s: Invalid MTU.\n",
+		VELOCITY_PRT(MSG_LEVEL_ERR, KERN_NOTICE "%s: Invalid MTU.\n", 
 				vptr->dev->name);
 		return -EINVAL;
 	}
 
-	if (!netif_running(dev)) {
-		dev->mtu = new_mtu;
-		return 0;
-	}
-
 	if (new_mtu != oldmtu) {
 		spin_lock_irqsave(&vptr->lock, flags);
 
@@ -1978,7 +1958,7 @@
 		velocity_free_td_ring(vptr);
 		velocity_free_rd_ring(vptr);
 
-		dev->mtu = new_mtu;
+		dev->mtu = new_mtu + NET_IP_ALIGN + EXTRA_RX_SKB_SPACE;
 
 		ret = velocity_init_rd_ring(vptr);
 		if (ret < 0)
@@ -2006,7 +1986,7 @@
  *	Shuts down the internal operations of the velocity and
  *	disables interrupts, autopolling, transmit and receive
  */
-
+ 
 static void velocity_shutdown(struct velocity_info *vptr)
 {
 	struct mac_regs __iomem * regs = vptr->mac_regs;
@@ -2037,10 +2017,10 @@
 		velocity_get_ip(vptr);
 	if (dev->irq != 0)
 		free_irq(dev->irq, dev);
-
+		
 	/* Power down the chip */
 	pci_set_power_state(vptr->pdev, PCI_D3hot);
-
+	
 	/* Free the resources */
 	velocity_free_td_ring(vptr);
 	velocity_free_rd_ring(vptr);
@@ -2058,7 +2038,7 @@
  *	Called by the networ layer to request a packet is queued to
  *	the velocity. Returns zero on success.
  */
-
+ 
 static int velocity_xmit(struct sk_buff *skb, struct net_device *dev)
 {
 	struct velocity_info *vptr = netdev_priv(dev);
@@ -2196,8 +2203,10 @@
 		td_ptr->td_buf[0].queue = 1;
 		mac_tx_queue_wake(vptr->mac_regs, qnum);
 	}
+
 	dev->trans_start = jiffies;
 	spin_unlock_irqrestore(&vptr->lock, flags);
+printk("xT\n");
 	return 0;
 }
 
@@ -2205,57 +2214,55 @@
  *	velocity_intr		-	interrupt callback
  *	@irq: interrupt number
  *	@dev_instance: interrupting device
+ *	@pt_regs: CPU register state at interrupt
  *
  *	Called whenever an interrupt is generated by the velocity
  *	adapter IRQ line. We may not be the source of the interrupt
  *	and need to identify initially if we are, and if not exit as
  *	efficiently as possible.
  */
-
-static int velocity_intr(int irq, void *dev_instance)
+ 
+static int velocity_intr(int irq, void *dev_instance, struct pt_regs *regs)
 {
 	struct net_device *dev = dev_instance;
 	struct velocity_info *vptr = netdev_priv(dev);
 	u32 isr_status;
 	int max_count = 0;
 
-
+printk("I");
 	spin_lock(&vptr->lock);
 	isr_status = mac_read_isr(vptr->mac_regs);
+printk("0x%08x ", isr_status);
 
-	/* Not us ? */
-	if (isr_status == 0) {
+	if (!isr_status) {
 		spin_unlock(&vptr->lock);
+printk("N ");
 		return IRQ_NONE;
 	}
 
 	mac_disable_int(vptr->mac_regs);
 
-	/*
-	 *	Keep processing the ISR until we have completed
-	 *	processing and the isr_status becomes zero
-	 */
-
-	while (isr_status != 0) {
+	/* Process all pending interrupts */
+	while (isr_status) {
+        /* Ack all pending interrupt sources */
 		mac_write_isr(vptr->mac_regs, isr_status);
-		if (isr_status & (~(ISR_PRXI | ISR_PPRXI | ISR_PTXI | ISR_PPTXI)))
-			velocity_error(vptr, isr_status);
-		if (isr_status & (ISR_PRXI | ISR_PPRXI))
+
+		velocity_error(vptr, isr_status);
+		if (isr_status & (ISR_PRXI | ISR_PPRXI)) {
+printk("R");
 			max_count += velocity_rx_srv(vptr, isr_status);
-		if (isr_status & (ISR_PTXI | ISR_PPTXI))
+		}
+		if (isr_status & (ISR_PTXI | ISR_PPTXI)) {
+printk("T");
 			max_count += velocity_tx_srv(vptr, isr_status);
-		isr_status = mac_read_isr(vptr->mac_regs);
-		if (max_count > vptr->options.int_works)
-		{
-			printk(KERN_WARNING "%s: excessive work at interrupt.\n",
-				dev->name);
-			max_count = 0;
 		}
+		isr_status = mac_read_isr(vptr->mac_regs);
 	}
+
 	spin_unlock(&vptr->lock);
+printk("i");
 	mac_enable_int(vptr->mac_regs);
 	return IRQ_HANDLED;
-
 }
 
 
@@ -2267,41 +2274,70 @@
  *	for a velocity adapter. Reload the CAMs with the new address
  *	filter ruleset.
  */
+static void clear_all_multicast(struct velocity_info *vptr)
+{
+	struct mac_regs __iomem * regs = vptr->mac_regs;
+
+    /* Do not allow any multicast packets through the multicast hash table */
+    writel(0x0, &regs->MARCAM[0]);
+    writel(0x0, &regs->MARCAM[4]);
+}
+
+static void set_all_multicast(struct velocity_info *vptr)
+{
+	struct mac_regs __iomem * regs = vptr->mac_regs;
+
+    /* Allow all multicast packets through the multicast hash table */
+    writel(0xffffffff, &regs->MARCAM[0]);
+    writel(0xffffffff, &regs->MARCAM[4]);
+}
 
 static void velocity_set_multi(struct net_device *dev)
 {
 	struct velocity_info *vptr = netdev_priv(dev);
 	struct mac_regs __iomem * regs = vptr->mac_regs;
-	u8 rx_mode;
-	int i;
 	struct dev_mc_list *mclist;
+	int i;
+	u8 rx_mode = RCR_AB;    /* Always enable broadcast packet reception */
 
-	if (dev->flags & IFF_PROMISC) {	/* Set promiscuous. */
-		writel(0xffffffff, &regs->MARCAM[0]);
-		writel(0xffffffff, &regs->MARCAM[4]);
-		rx_mode = (RCR_AM | RCR_AB | RCR_PROM);
-	} else if ((dev->mc_count > vptr->multicast_limit)
-		   || (dev->flags & IFF_ALLMULTI)) {
-		writel(0xffffffff, &regs->MARCAM[0]);
-		writel(0xffffffff, &regs->MARCAM[4]);
-		rx_mode = (RCR_AM | RCR_AB);
-	} else {
-		int offset = MCAM_SIZE - vptr->multicast_limit;
-		mac_get_cam_mask(regs, vptr->mCAMmask);
+    /* Clear out the multicast hash table */
+    clear_all_multicast(vptr);
 
+    /* Disable all multicast CAM entries */
+	memset(vptr->mCAMmask, 0, MCAM_SIZE / 8);
+    mac_set_cam_mask(regs, vptr->mCAMmask, VELOCITY_MULTICAST_CAM);
+
+	if (dev->flags & IFF_PROMISC) {
+        /* Enable promiscuous mode */
+		printk(KERN_NOTICE "%s: Promiscuous mode enabled.\n", dev->name);
+
+        set_all_multicast(vptr);
+		rx_mode |= (RCR_PROM | RCR_AM);
+	} else if ((dev->mc_count > MCAM_SIZE) ||
+               (dev->flags & IFF_ALLMULTI)) {
+        /* ALL_MULTI or too many entries for perfect filtering, so allow all
+         * multicast packets through hash table */
+        set_all_multicast(vptr);
+		rx_mode |= RCR_AM;
+	} else {
+        /* Write a CAM entry for each multicast address */
 		for (i = 0, mclist = dev->mc_list; mclist && i < dev->mc_count; i++, mclist = mclist->next) {
-			mac_set_cam(regs, i + offset, mclist->dmi_addr);
-			vptr->mCAMmask[(offset + i) / 8] |= 1 << ((offset + i) & 7);
+			mac_set_cam(regs, i, mclist->dmi_addr, VELOCITY_MULTICAST_CAM);
+			vptr->mCAMmask[i / 8] |= 1 << (i & 7);
 		}
 
-		mac_set_cam_mask(regs, vptr->mCAMmask);
-		rx_mode = (RCR_AM | RCR_AB);
+        /* Enable those multicast CAM entries just written */
+		mac_set_cam_mask(regs, vptr->mCAMmask, VELOCITY_MULTICAST_CAM);
+
+        /* Enable multicast perfect matching */
+		rx_mode |= (RCR_AM | RCR_AP);
 	}
+
+    /* Allow large packets if indicated by MTU */
 	if (dev->mtu > 1500)
 		rx_mode |= RCR_AL;
 
-	BYTE_REG_BITS_ON(rx_mode, &regs->RCR);
-
+	BYTE_REG_BITS_SET(rx_mode, RCR_AM | RCR_AB | RCR_PROM | RCR_AL | RCR_AP, &regs->RCR);
 }
 
 /**
@@ -2314,36 +2350,32 @@
  *	the hardware into the counters before letting the network
  *	layer display them.
  */
-
+ 
 static struct net_device_stats *velocity_get_stats(struct net_device *dev)
 {
-	struct velocity_info *vptr = netdev_priv(dev);
+	struct velocity_info *vptr = dev->priv;
 
-	/* If the hardware is down, don't touch MII */
-	if(!netif_running(dev))
-		return &vptr->stats;
-
-	spin_lock_irq(&vptr->lock);
-	velocity_update_hw_mibs(vptr);
-	spin_unlock_irq(&vptr->lock);
-
-	vptr->stats.rx_packets = vptr->mib_counter[HW_MIB_ifRxAllPkts];
-	vptr->stats.rx_errors = vptr->mib_counter[HW_MIB_ifRxErrorPkts];
-	vptr->stats.rx_length_errors = vptr->mib_counter[HW_MIB_ifInRangeLengthErrors];
-
-//  unsigned long   rx_dropped;     /* no space in linux buffers    */
-	vptr->stats.collisions = vptr->mib_counter[HW_MIB_ifTxEtherCollisions];
-	/* detailed rx_errors: */
-//  unsigned long   rx_length_errors;
-//  unsigned long   rx_over_errors;     /* receiver ring buff overflow  */
-	vptr->stats.rx_crc_errors = vptr->mib_counter[HW_MIB_ifRxPktCRCE];
-//  unsigned long   rx_frame_errors;    /* recv'd frame alignment error */
-//  unsigned long   rx_fifo_errors;     /* recv'r fifo overrun      */
-//  unsigned long   rx_missed_errors;   /* receiver missed packet   */
+#if 0
+    /* Only access the MIBs if the hardware is up */
+    if (netif_running(dev)) {
+        int i;
+
+        /* Transfer from fast counters to MIB SRAM, locking against
+           simulatanous access by ISR due to MIB high threshold being
+           exceeded */
+        spin_lock_irq(&vptr->lock);
+        velocity_update_hw_mibs(vptr);
+        spin_unlock_irq(&vptr->lock);
 
-	/* detailed tx_errors */
-//  unsigned long   tx_fifo_errors;
+        /* Print MIB statistics */
+        printk("MIBs:\n");
+        for (i=0; i < HW_MIB_SIZE; ++i) {
+            printk("%02d: %08d\n", i, vptr->mib_counter[i]);
+        }
+    }
+#endif
 
+    /* Return only the statistics gathered by the driver, not MIBs */
 	return &vptr->stats;
 }
 
@@ -2357,7 +2389,7 @@
  *	Called when the user issues an ioctl request to the network
  *	device in question. The velocity interface supports MII.
  */
-
+ 
 static int velocity_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
 {
 	struct velocity_info *vptr = netdev_priv(dev);
@@ -2365,10 +2397,10 @@
 
 	/* If we are asked for information and the device is power
 	   saving then we need to bring the device back up to talk to it */
-
+	   	
 	if (!netif_running(dev))
 		pci_set_power_state(vptr->pdev, PCI_D0);
-
+		
 	switch (cmd) {
 	case SIOCGMIIPHY:	/* Get address of MII PHY in use. */
 	case SIOCGMIIREG:	/* Read MII PHY register. */
@@ -2381,8 +2413,8 @@
 	}
 	if (!netif_running(dev))
 		pci_set_power_state(vptr->pdev, PCI_D3hot);
-
-
+		
+		
 	return ret;
 }
 
@@ -2390,7 +2422,7 @@
  *	Definition for our device driver. The PCI layer interface
  *	uses this to handle all our card discover and plugging
  */
-
+ 
 static struct pci_driver velocity_driver = {
       .name	= VELOCITY_NAME,
       .id_table	= velocity_id_table,
@@ -2410,13 +2442,13 @@
  *	the probe functions for each velocity adapter installed
  *	in the system.
  */
-
+ 
 static int __init velocity_init_module(void)
 {
 	int ret;
 
 	velocity_register_notifier();
-	ret = pci_register_driver(&velocity_driver);
+	ret = pci_module_init(&velocity_driver);
 	if (ret < 0)
 		velocity_unregister_notifier();
 	return ret;
@@ -2426,11 +2458,11 @@
  *	velocity_cleanup	-	module unload
  *
  *	When the velocity hardware is unloaded this function is called.
- *	It will clean up the notifiers and the unregister the PCI
+ *	It will clean up the notifiers and the unregister the PCI 
  *	driver interface for this hardware. This in turn cleans up
  *	all discovered interfaces before returning from the function
  */
-
+ 
 static void __exit velocity_cleanup_module(void)
 {
 	velocity_unregister_notifier();
@@ -2444,8 +2476,8 @@
 /*
  * MII access , media link mode setting functions
  */
-
-
+ 
+ 
 /**
  *	mii_init	-	set up MII
  *	@vptr: velocity adapter
@@ -2453,7 +2485,7 @@
  *
  *	Set up the PHY for the current link state.
  */
-
+ 
 static void mii_init(struct velocity_info *vptr, u32 mii_status)
 {
 	u16 BMCR;
@@ -2466,7 +2498,7 @@
 		MII_REG_BITS_OFF((ANAR_ASMDIR | ANAR_PAUSE), MII_REG_ANAR, vptr->mac_regs);
 		/*
 		 *	Turn on ECHODIS bit in NWay-forced full mode and turn it
-		 *	off it in NWay-forced half mode for NWay-forced v.s.
+		 *	off it in NWay-forced half mode for NWay-forced v.s. 
 		 *	legacy-forced issue.
 		 */
 		if (vptr->mii_status & VELOCITY_DUPLEX_FULL)
@@ -2486,7 +2518,7 @@
 		MII_REG_BITS_ON((ANAR_ASMDIR | ANAR_PAUSE), MII_REG_ANAR, vptr->mac_regs);
 		/*
 		 *	Turn on ECHODIS bit in NWay-forced full mode and turn it
-		 *	off it in NWay-forced half mode for NWay-forced v.s.
+		 *	off it in NWay-forced half mode for NWay-forced v.s. 
 		 *	legacy-forced issue
 		 */
 		if (vptr->mii_status & VELOCITY_DUPLEX_FULL)
@@ -2498,11 +2530,11 @@
 	case PHYID_MARVELL_1000:
 	case PHYID_MARVELL_1000S:
 		/*
-		 *	Assert CRS on Transmit
+		 *	Assert CRS on Transmit 
 		 */
 		MII_REG_BITS_ON(PSCR_ACRSTX, MII_REG_PSCR, vptr->mac_regs);
 		/*
-		 *	Reset to hardware default
+		 *	Reset to hardware default 
 		 */
 		MII_REG_BITS_ON((ANAR_ASMDIR | ANAR_PAUSE), MII_REG_ANAR, vptr->mac_regs);
 		break;
@@ -2522,7 +2554,7 @@
  *
  *	Turn off the autopoll and wait for it to disable on the chip
  */
-
+ 
 static void safe_disable_mii_autopoll(struct mac_regs __iomem * regs)
 {
 	u16 ww;
@@ -2576,7 +2608,7 @@
  *	Perform a single read of an MII 16bit register. Returns zero
  *	on success or -ETIMEDOUT if the PHY did not respond.
  */
-
+ 
 static int velocity_mii_read(struct mac_regs __iomem *regs, u8 index, u16 *data)
 {
 	u16 ww;
@@ -2612,7 +2644,7 @@
  *	Perform a single write to an MII 16bit register. Returns zero
  *	on success or -ETIMEDOUT if the PHY did not respond.
  */
-
+ 
 static int velocity_mii_write(struct mac_regs __iomem *regs, u8 mii_addr, u16 data)
 {
 	u16 ww;
@@ -2651,7 +2683,7 @@
  *	mii_status accordingly. The requested link state information
  *	is also returned.
  */
-
+ 
 static u32 velocity_get_opt_media_mode(struct velocity_info *vptr)
 {
 	u32 status = 0;
@@ -2683,7 +2715,7 @@
  *
  *	Enable autonegotation on this interface
  */
-
+ 
 static void mii_set_auto_on(struct velocity_info *vptr)
 {
 	if (MII_REG_BITS_IS_ON(BMCR_AUTO, MII_REG_BMCR, vptr->mac_regs))
@@ -2707,7 +2739,7 @@
  *	Set up the flow control on this interface according to
  *	the supplied user/eeprom options.
  */
-
+ 
 static void set_mii_flow_control(struct velocity_info *vptr)
 {
 	/*Enable or Disable PAUSE in ANAR */
@@ -2744,7 +2776,7 @@
  *	PHY and also velocity hardware setup accordingly. In particular
  *	we need to set up CD polling and frame bursting.
  */
-
+ 
 static int velocity_set_media_mode(struct velocity_info *vptr, u32 mii_status)
 {
 	u32 curr_status;
@@ -2854,7 +2886,7 @@
  *	Check the current MII status and determine the link status
  *	accordingly
  */
-
+ 
 static u32 mii_check_media_mode(struct mac_regs __iomem * regs)
 {
 	u32 status = 0;
@@ -2986,14 +3018,14 @@
  *	Called before an ethtool operation. We need to make sure the
  *	chip is out of D3 state before we poke at it.
  */
-
+ 
 static int velocity_ethtool_up(struct net_device *dev)
 {
 	struct velocity_info *vptr = netdev_priv(dev);
 	if (!netif_running(dev))
 		pci_set_power_state(vptr->pdev, PCI_D0);
 	return 0;
-}
+}	
 
 /**
  *	velocity_ethtool_down	-	post hook for ethtool
@@ -3002,7 +3034,7 @@
  *	Called after an ethtool operation. Restore the chip back to D3
  *	state if it isn't running.
  */
-
+ 
 static void velocity_ethtool_down(struct net_device *dev)
 {
 	struct velocity_info *vptr = netdev_priv(dev);
@@ -3040,7 +3072,21 @@
 		cmd->duplex = DUPLEX_FULL;
 	else
 		cmd->duplex = DUPLEX_HALF;
-
+		
+#if 0
+{
+int i;
+unsigned char __iomem *ptr = (unsigned char __iomem *)regs;
+printk("Regs:\n");
+printk("0x00:\t");
+for (i=0; i <= MAC_REG_BYTEMSK3_3; ++i) {
+    printk("0x%02x\t", readb(&(ptr[i])));
+    if (!((i+1) % 8)) {
+        printk("\n0x%02x:\t", i+1);
+    }
+}
+}
+#endif
 	return 0;
 }
 
@@ -3050,7 +3096,7 @@
 	u32 curr_status;
 	u32 new_status = 0;
 	int ret = 0;
-
+	
 	curr_status = check_connection_type(vptr->mac_regs);
 	curr_status &= (~VELOCITY_LINK_FAIL);
 
@@ -3061,8 +3107,10 @@
 
 	if ((new_status & VELOCITY_AUTONEG_ENABLE) && (new_status != (curr_status | VELOCITY_AUTONEG_ENABLE)))
 		ret = -EINVAL;
-	else
+	else {
 		velocity_set_media_mode(vptr, new_status);
+		mac_set_dma_length(vptr);
+    }
 
 	return ret;
 }
@@ -3139,7 +3187,7 @@
 	 msglevel = value;
 }
 
-static const struct ethtool_ops velocity_ethtool_ops = {
+static struct ethtool_ops velocity_ethtool_ops = {
 	.get_settings	=	velocity_get_settings,
 	.set_settings	=	velocity_set_settings,
 	.get_drvinfo	=	velocity_get_drvinfo,
@@ -3162,7 +3210,7 @@
  *	are used by tools like kudzu to interrogate the link state of the
  *	hardware
  */
-
+ 
 static int velocity_mii_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
 {
 	struct velocity_info *vptr = netdev_priv(dev);
@@ -3170,7 +3218,7 @@
 	unsigned long flags;
 	struct mii_ioctl_data *miidata = if_mii(ifr);
 	int err;
-
+	
 	switch (cmd) {
 	case SIOCGMIIPHY:
 		miidata->phy_id = readb(&regs->MIIADR) & 0x1f;
@@ -3197,11 +3245,36 @@
 	return 0;
 }
 
+static void hw_set_mac_address(struct velocity_info *vptr, unsigned char* p)
+{
+    struct mac_regs __iomem * regs = vptr->mac_regs;
+
+    int i;
+    for (i = 0; i < 6; i++) {
+        writeb(p[i], &(regs->PAR[i]));
+    }
+}
+
+static int set_mac_address(struct net_device *dev, void *p)
+{
+    struct sockaddr *addr = p;
+    struct velocity_info *vptr = dev->priv;
+
+    if (!is_valid_ether_addr(addr->sa_data)) {
+        return -EADDRNOTAVAIL;
+    }
+
+    memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
+    hw_set_mac_address(vptr, addr->sa_data);
+
+    return 0;
+}
+
 #ifdef CONFIG_PM
 
 /**
  *	velocity_save_context	-	save registers
- *	@vptr: velocity
+ *	@vptr: velocity 
  *	@context: buffer for stored context
  *
  *	Retrieve the current configuration from the velocity hardware
@@ -3209,7 +3282,7 @@
  *	restore functions. This allows us to save things we need across
  *	power down states
  */
-
+ 
 static void velocity_save_context(struct velocity_info *vptr, struct velocity_context * context)
 {
 	struct mac_regs __iomem * regs = vptr->mac_regs;
@@ -3229,13 +3302,13 @@
 
 /**
  *	velocity_restore_context	-	restore registers
- *	@vptr: velocity
+ *	@vptr: velocity 
  *	@context: buffer for stored context
  *
- *	Reload the register configuration from the velocity context
+ *	Reload the register configuration from the velocity context 
  *	created by velocity_save_context.
  */
-
+ 
 static void velocity_restore_context(struct velocity_info *vptr, struct velocity_context *context)
 {
 	struct mac_regs __iomem * regs = vptr->mac_regs;
@@ -3301,7 +3374,7 @@
 	}
 	/*	Finally, invert the result once to get the correct data */
 	crc = ~crc;
-	return bitrev32(crc) >> 16;
+	return bitreverse(crc) >> 16;
 }
 
 /**
@@ -3461,8 +3534,6 @@
 	return 0;
 }
 
-#ifdef CONFIG_INET
-
 static int velocity_netdev_event(struct notifier_block *nb, unsigned long notification, void *ptr)
 {
 	struct in_ifaddr *ifa = (struct in_ifaddr *) ptr;
@@ -3483,6 +3554,4 @@
 	}
 	return NOTIFY_DONE;
 }
-
-#endif
 #endif
